{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load les données\n",
    "\n",
    "fichier = open('villes.txt')\n",
    "donnees = fichier.read()\n",
    "villes = donnees.replace('\\n', ',').split(',')\n",
    "villes = [ville for ville in villes if len(ville) > 2]\n",
    "villes = sorted(villes, key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# création du vocabulaire\n",
    "\n",
    "vocabulaire = sorted(list(set(''.join(villes))))\n",
    "vocabulaire = [\"<SOS>\", \"<EOS>\"] + vocabulaire\n",
    "\n",
    "# pour convertir char <-> int\n",
    "char_to_int = {}\n",
    "int_to_char = {}\n",
    "\n",
    "for (c, i) in zip(vocabulaire, range(len(vocabulaire))):\n",
    "    char_to_int[c] = i\n",
    "    int_to_char[i] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sequences = len(villes)\n",
    "max_len = max([len(ville) for ville in villes]) + 2 # account for <SOS> and <EOS>\n",
    "\n",
    "X = torch.zeros((num_sequences, max_len))\n",
    "\n",
    "for i in range(num_sequences):\n",
    "    X[i] = torch.tensor([char_to_int['<SOS>']] + [char_to_int[c] for c in villes[i]] + [char_to_int['<EOS>']] + [-1] * (max_len - len(villes[i]) - 2))\n",
    "\n",
    "n_split = int(0.9*X.shape[0])\n",
    "\n",
    "X_train = X[:n_split]\n",
    "X_val = X[n_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split, batch_size):\n",
    "    # returns a batch, according to the data pipeline written in the W&B report\n",
    "\n",
    "    idx_seed = torch.randint(high=X_train.shape[0], size=(1,)).item() #sample la ligne seed autour de laquelle on va piocher les exemples\n",
    "\n",
    "    idx = torch.randint(low = max(0, idx_seed - 4 * batch_size), high = min(X_train.shape[0], idx_seed + 4 * batch_size), size=(batch_size,)) #samples les indices du batch à produire\n",
    "    #pq 4 ? bon compromis entre assez large pour pas bcp de répétitions, assez petit pour pas bcp de padding (cf data.ipynb)\n",
    "\n",
    "    idx_sorted, _ = torch.sort(idx) #on les ordonne pour recuperer facilement la longueur de la plus grande seq. du batch\n",
    "\n",
    "    X_batch = X_train[idx_sorted] #on extrait la matrice qui va produire Xb et Yb\n",
    "\n",
    "    max_len_batch = torch.sum(torch.ne(X_batch[-1], -1)) #longueur de la plus grande seq. du batch : torch.ne(X_batch[-1], -1) crée une matrice masque, avec True si diff de -1, False si egal a -1\n",
    "\n",
    "    Xb = X_batch[:, :max_len_batch-1] #on selectionne que jusqu'a la len max - 1 (<EOS> du plus long inutile) (le reste n'est que padding)\n",
    "    Yb = X_batch[:, 1:max_len_batch] #meme que Xb, mais décalé de 1 (avec le <EOS> mais sans le <SOS>)\n",
    "\n",
    "    #Xb[Xb == 1] = -1 #on remplace le <EOS> par du padding (totalement optionnel)\n",
    "\n",
    "    return Xb.pin_memory().to('cuda', non_blocking=True), Yb.pin_memory().to('cuda', non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.03\n",
    "batch_size = 1024\n",
    "embed_dim = 16\n",
    "hidden_dim = 100\n",
    "context_len = 3\n",
    "data = \"new\"\n",
    "\n",
    "eval_interval = 500\n",
    "eval_iter = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BengioLM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed = nn.Embedding(len(vocabulaire), embed_dim)\n",
    "        self.fc1 = nn.Linear(context_len * embed_dim, hidden_dim)\n",
    "\n",
    "        self.lm_head = nn.Linear(hidden_dim, len(vocabulaire))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x).view(-1, context_len*embed_dim)\n",
    "\n",
    "        z1 = self.fc1(x)\n",
    "        a1 = F.tanh(z1)\n",
    "\n",
    "        logits = self.lm_head(a1)\n",
    "\n",
    "        return z1, a1, logits\n",
    "    \n",
    "    def sample(self, prompt, max_new_tokens):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xb, Yb = get_batch('train', 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  7., 12., 32., 24., 13., 16., 16., 19., 18.],\n",
       "        [ 0.,  6.,  5., 18., 18., 13., 34., 22.,  9., 23.],\n",
       "        [ 0., 16.,  9.,  2.,  6.,  5., 22.,  8., 19., 18.],\n",
       "        [ 0., 10., 19., 25., 22.,  8., 22.,  5., 13., 18.],\n",
       "        [ 0., 16.,  9.,  2., 20.,  5., 13., 16., 16., 29.],\n",
       "        [ 0., 25., 12., 22., 27., 13., 16., 16.,  9., 22.],\n",
       "        [ 0.,  7., 19., 22.,  6., 13., 34., 22.,  9., 23.],\n",
       "        [ 0.,  5., 13., 11., 25., 22.,  5., 18.,  8.,  9.],\n",
       "        [ 0.,  5., 17.,  6., 22., 13., 34., 22.,  9., 23.],\n",
       "        [ 0., 22.,  5., 22., 35.,  7., 19., 25., 22., 24.],\n",
       "        [ 0., 16., 19., 18., 11., 10., 19., 23., 23., 35.],\n",
       "        [ 0., 17., 19., 18., 24.,  7., 12.,  9., 18., 25.],\n",
       "        [ 0., 17.,  5., 22., 23.,  5., 18.,  9., 13., 28.],\n",
       "        [ 0., 17.,  5., 22., 23.,  5., 18.,  9., 13., 28.],\n",
       "        [ 0., 23.,  9., 20., 17.,  9., 22., 13.,  9., 23.],\n",
       "        [ 0., 17.,  5., 16., 16., 13., 34., 26., 22.,  9.],\n",
       "        [ 0., 20., 25., 29., 17.,  5., 25., 22., 13., 18.],\n",
       "        [ 0., 10., 16., 19., 25., 22., 23., 13.,  9., 23.],\n",
       "        [ 0., 26.,  5.,  7., 21., 25.,  9., 22., 13.,  9.],\n",
       "        [ 0., 23.,  9., 22., 26., 13., 34., 22.,  9., 23.],\n",
       "        [ 0., 17., 35., 22., 13.,  7., 19., 25., 22., 24.],\n",
       "        [ 0., 23.,  5., 26., 13., 11., 18., 13.,  9., 23.],\n",
       "        [ 0., 23.,  5., 16., 13., 18.,  8., 22.,  9., 23.],\n",
       "        [ 0.,  6., 22., 13.,  5., 24.,  9., 28., 24.,  9.],\n",
       "        [ 0.,  6., 22., 13.,  5., 24.,  9., 28., 24.,  9.],\n",
       "        [ 0., 21., 25., 13., 18.,  7.,  9., 22., 19., 24.],\n",
       "        [ 0., 11., 19., 25.,  9., 23., 18.,  5.,  7., 12.],\n",
       "        [ 0.,  7., 12.,  5., 30.,  9., 16., 16.,  9., 23.],\n",
       "        [ 0., 17., 19., 22., 17.,  5., 13., 23., 19., 18.],\n",
       "        [ 0., 23., 24., 19., 24., 30., 12.,  9., 13., 17.],\n",
       "        [ 0.,  6., 25., 22.,  8., 13., 11., 18., 13., 18.],\n",
       "        [ 0.,  7., 19., 22.,  6., 13., 34., 22.,  9., 23.],\n",
       "        [ 0.,  5., 25.,  6.,  9., 26., 13., 16., 16.,  9.],\n",
       "        [ 0., 26.,  5., 16., 19., 14., 19., 25., 16., 28.],\n",
       "        [ 0., 26., 13., 16., 16.,  9.,  8., 13.,  9., 25.],\n",
       "        [ 0., 17.,  5., 13., 16., 16., 34., 22.,  9., 23.],\n",
       "        [ 0., 17.,  5., 13., 16., 16., 34., 22.,  9., 23.],\n",
       "        [ 0., 22., 13., 21., 25.,  9., 27., 13., 12., 22.],\n",
       "        [ 0., 16.,  9., 18.,  7., 19., 25.,  5.,  7., 21.],\n",
       "        [ 0., 16.,  9.,  2., 17., 19., 18., 24.,  9., 24.],\n",
       "        [ 0., 26., 13., 16., 16.,  9.,  6., 19., 25., 24.],\n",
       "        [ 0., 13., 22., 22.,  9., 26., 13., 16., 16.,  9.],\n",
       "        [ 0., 22.,  9.,  7., 19., 16., 19., 11., 18.,  9.],\n",
       "        [ 0., 13., 18., 23., 26., 13., 16., 16.,  9., 22.],\n",
       "        [ 0., 18.,  9., 25., 10., 16., 13.,  9., 25., 28.],\n",
       "        [ 0., 23., 13., 18., 18.,  5., 17.,  5., 22., 29.],\n",
       "        [ 0., 13., 22., 13., 23., 23.,  5., 22., 22., 29.],\n",
       "        [ 0., 26.,  9., 22., 30.,  9., 13., 16., 16.,  9.],\n",
       "        [ 0., 16.,  9.,  2., 22.,  9.,  7., 19., 25., 28.],\n",
       "        [ 0., 17., 19., 18., 24.,  5., 29., 22.,  5., 16.],\n",
       "        [ 0., 16., 19., 25., 26., 22.,  9.,  7., 12., 29.],\n",
       "        [ 0., 26., 19., 25., 16., 24.,  9., 11., 19., 18.],\n",
       "        [ 0., 20.,  9., 16., 16.,  9., 11., 22., 25.,  9.],\n",
       "        [ 0.,  6., 22., 19., 23., 26., 13., 16., 16.,  9.],\n",
       "        [ 0., 23.,  9., 13., 11., 18.,  9., 16.,  5., 29.],\n",
       "        [ 0.,  6., 19., 13., 23., 23.,  9., 30., 19., 18.],\n",
       "        [ 0.,  6.,  5., 22.,  6., 13., 34., 22.,  9., 23.],\n",
       "        [ 0.,  7., 19., 17.,  6.,  9., 22., 14., 19., 18.],\n",
       "        [ 0., 20.,  9., 29., 22.,  9.,  7.,  5., 26.,  9.],\n",
       "        [ 0., 20., 16., 35.,  6., 19., 25., 16., 16.,  9.],\n",
       "        [ 0.,  5., 17.,  6., 16., 13., 17., 19., 18., 24.],\n",
       "        [ 0., 35.,  7.,  5., 21., 25.,  9., 16., 19., 18.],\n",
       "        [ 0., 24., 22.,  9., 23., 23.,  5., 18., 11.,  9.],\n",
       "        [ 0., 16.,  9.,  2., 17.,  5., 18., 19., 13., 22.]], device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7., 12., 32., 24., 13., 16., 16., 19., 18.,  1.],\n",
       "        [ 6.,  5., 18., 18., 13., 34., 22.,  9., 23.,  1.],\n",
       "        [16.,  9.,  2.,  6.,  5., 22.,  8., 19., 18.,  1.],\n",
       "        [10., 19., 25., 22.,  8., 22.,  5., 13., 18.,  1.],\n",
       "        [16.,  9.,  2., 20.,  5., 13., 16., 16., 29.,  1.],\n",
       "        [25., 12., 22., 27., 13., 16., 16.,  9., 22.,  1.],\n",
       "        [ 7., 19., 22.,  6., 13., 34., 22.,  9., 23.,  1.],\n",
       "        [ 5., 13., 11., 25., 22.,  5., 18.,  8.,  9.,  1.],\n",
       "        [ 5., 17.,  6., 22., 13., 34., 22.,  9., 23.,  1.],\n",
       "        [22.,  5., 22., 35.,  7., 19., 25., 22., 24.,  1.],\n",
       "        [16., 19., 18., 11., 10., 19., 23., 23., 35.,  1.],\n",
       "        [17., 19., 18., 24.,  7., 12.,  9., 18., 25.,  1.],\n",
       "        [17.,  5., 22., 23.,  5., 18.,  9., 13., 28.,  1.],\n",
       "        [17.,  5., 22., 23.,  5., 18.,  9., 13., 28.,  1.],\n",
       "        [23.,  9., 20., 17.,  9., 22., 13.,  9., 23.,  1.],\n",
       "        [17.,  5., 16., 16., 13., 34., 26., 22.,  9.,  1.],\n",
       "        [20., 25., 29., 17.,  5., 25., 22., 13., 18.,  1.],\n",
       "        [10., 16., 19., 25., 22., 23., 13.,  9., 23.,  1.],\n",
       "        [26.,  5.,  7., 21., 25.,  9., 22., 13.,  9.,  1.],\n",
       "        [23.,  9., 22., 26., 13., 34., 22.,  9., 23.,  1.],\n",
       "        [17., 35., 22., 13.,  7., 19., 25., 22., 24.,  1.],\n",
       "        [23.,  5., 26., 13., 11., 18., 13.,  9., 23.,  1.],\n",
       "        [23.,  5., 16., 13., 18.,  8., 22.,  9., 23.,  1.],\n",
       "        [ 6., 22., 13.,  5., 24.,  9., 28., 24.,  9.,  1.],\n",
       "        [ 6., 22., 13.,  5., 24.,  9., 28., 24.,  9.,  1.],\n",
       "        [21., 25., 13., 18.,  7.,  9., 22., 19., 24.,  1.],\n",
       "        [11., 19., 25.,  9., 23., 18.,  5.,  7., 12.,  1.],\n",
       "        [ 7., 12.,  5., 30.,  9., 16., 16.,  9., 23.,  1.],\n",
       "        [17., 19., 22., 17.,  5., 13., 23., 19., 18.,  1.],\n",
       "        [23., 24., 19., 24., 30., 12.,  9., 13., 17.,  1.],\n",
       "        [ 6., 25., 22.,  8., 13., 11., 18., 13., 18.,  1.],\n",
       "        [ 7., 19., 22.,  6., 13., 34., 22.,  9., 23.,  1.],\n",
       "        [ 5., 25.,  6.,  9., 26., 13., 16., 16.,  9.,  1.],\n",
       "        [26.,  5., 16., 19., 14., 19., 25., 16., 28.,  1.],\n",
       "        [26., 13., 16., 16.,  9.,  8., 13.,  9., 25.,  1.],\n",
       "        [17.,  5., 13., 16., 16., 34., 22.,  9., 23.,  1.],\n",
       "        [17.,  5., 13., 16., 16., 34., 22.,  9., 23.,  1.],\n",
       "        [22., 13., 21., 25.,  9., 27., 13., 12., 22.,  1.],\n",
       "        [16.,  9., 18.,  7., 19., 25.,  5.,  7., 21.,  1.],\n",
       "        [16.,  9.,  2., 17., 19., 18., 24.,  9., 24.,  1.],\n",
       "        [26., 13., 16., 16.,  9.,  6., 19., 25., 24.,  1.],\n",
       "        [13., 22., 22.,  9., 26., 13., 16., 16.,  9.,  1.],\n",
       "        [22.,  9.,  7., 19., 16., 19., 11., 18.,  9.,  1.],\n",
       "        [13., 18., 23., 26., 13., 16., 16.,  9., 22.,  1.],\n",
       "        [18.,  9., 25., 10., 16., 13.,  9., 25., 28.,  1.],\n",
       "        [23., 13., 18., 18.,  5., 17.,  5., 22., 29.,  1.],\n",
       "        [13., 22., 13., 23., 23.,  5., 22., 22., 29.,  1.],\n",
       "        [26.,  9., 22., 30.,  9., 13., 16., 16.,  9.,  1.],\n",
       "        [16.,  9.,  2., 22.,  9.,  7., 19., 25., 28.,  1.],\n",
       "        [17., 19., 18., 24.,  5., 29., 22.,  5., 16.,  1.],\n",
       "        [16., 19., 25., 26., 22.,  9.,  7., 12., 29.,  1.],\n",
       "        [26., 19., 25., 16., 24.,  9., 11., 19., 18.,  1.],\n",
       "        [20.,  9., 16., 16.,  9., 11., 22., 25.,  9.,  1.],\n",
       "        [ 6., 22., 19., 23., 26., 13., 16., 16.,  9.,  1.],\n",
       "        [23.,  9., 13., 11., 18.,  9., 16.,  5., 29.,  1.],\n",
       "        [ 6., 19., 13., 23., 23.,  9., 30., 19., 18.,  1.],\n",
       "        [ 6.,  5., 22.,  6., 13., 34., 22.,  9., 23.,  1.],\n",
       "        [ 7., 19., 17.,  6.,  9., 22., 14., 19., 18.,  1.],\n",
       "        [20.,  9., 29., 22.,  9.,  7.,  5., 26.,  9.,  1.],\n",
       "        [20., 16., 35.,  6., 19., 25., 16., 16.,  9.,  1.],\n",
       "        [ 5., 17.,  6., 16., 13., 17., 19., 18., 24.,  1.],\n",
       "        [35.,  7.,  5., 21., 25.,  9., 16., 19., 18.,  1.],\n",
       "        [24., 22.,  9., 23., 23.,  5., 18., 11.,  9.,  1.],\n",
       "        [16.,  9.,  2., 17.,  5., 18., 19., 13., 22.,  1.]], device='cuda:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS>châtillon\n",
      "châtillon<EOS>\n",
      "**************************************************\n",
      "<SOS>bannières\n",
      "bannières<EOS>\n",
      "**************************************************\n",
      "<SOS>le bardon\n",
      "le bardon<EOS>\n",
      "**************************************************\n",
      "<SOS>fourdrain\n",
      "fourdrain<EOS>\n",
      "**************************************************\n",
      "<SOS>le pailly\n",
      "le pailly<EOS>\n",
      "**************************************************\n",
      "<SOS>uhrwiller\n",
      "uhrwiller<EOS>\n",
      "**************************************************\n",
      "<SOS>corbières\n",
      "corbières<EOS>\n",
      "**************************************************\n",
      "<SOS>aigurande\n",
      "aigurande<EOS>\n",
      "**************************************************\n",
      "<SOS>ambrières\n",
      "ambrières<EOS>\n",
      "**************************************************\n",
      "<SOS>rarécourt\n",
      "rarécourt<EOS>\n",
      "**************************************************\n",
      "<SOS>longfossé\n",
      "longfossé<EOS>\n",
      "**************************************************\n",
      "<SOS>montchenu\n",
      "montchenu<EOS>\n",
      "**************************************************\n",
      "<SOS>marsaneix\n",
      "marsaneix<EOS>\n",
      "**************************************************\n",
      "<SOS>marsaneix\n",
      "marsaneix<EOS>\n",
      "**************************************************\n",
      "<SOS>sepmeries\n",
      "sepmeries<EOS>\n",
      "**************************************************\n",
      "<SOS>mallièvre\n",
      "mallièvre<EOS>\n",
      "**************************************************\n",
      "<SOS>puymaurin\n",
      "puymaurin<EOS>\n",
      "**************************************************\n",
      "<SOS>floursies\n",
      "floursies<EOS>\n",
      "**************************************************\n",
      "<SOS>vacquerie\n",
      "vacquerie<EOS>\n",
      "**************************************************\n",
      "<SOS>servières\n",
      "servières<EOS>\n",
      "**************************************************\n",
      "<SOS>méricourt\n",
      "méricourt<EOS>\n",
      "**************************************************\n",
      "<SOS>savignies\n",
      "savignies<EOS>\n",
      "**************************************************\n",
      "<SOS>salindres\n",
      "salindres<EOS>\n",
      "**************************************************\n",
      "<SOS>briatexte\n",
      "briatexte<EOS>\n",
      "**************************************************\n",
      "<SOS>briatexte\n",
      "briatexte<EOS>\n",
      "**************************************************\n",
      "<SOS>quincerot\n",
      "quincerot<EOS>\n",
      "**************************************************\n",
      "<SOS>gouesnach\n",
      "gouesnach<EOS>\n",
      "**************************************************\n",
      "<SOS>chazelles\n",
      "chazelles<EOS>\n",
      "**************************************************\n",
      "<SOS>mormaison\n",
      "mormaison<EOS>\n",
      "**************************************************\n",
      "<SOS>stotzheim\n",
      "stotzheim<EOS>\n",
      "**************************************************\n",
      "<SOS>burdignin\n",
      "burdignin<EOS>\n",
      "**************************************************\n",
      "<SOS>corbières\n",
      "corbières<EOS>\n",
      "**************************************************\n",
      "<SOS>aubeville\n",
      "aubeville<EOS>\n",
      "**************************************************\n",
      "<SOS>valojoulx\n",
      "valojoulx<EOS>\n",
      "**************************************************\n",
      "<SOS>villedieu\n",
      "villedieu<EOS>\n",
      "**************************************************\n",
      "<SOS>maillères\n",
      "maillères<EOS>\n",
      "**************************************************\n",
      "<SOS>maillères\n",
      "maillères<EOS>\n",
      "**************************************************\n",
      "<SOS>riquewihr\n",
      "riquewihr<EOS>\n",
      "**************************************************\n",
      "<SOS>lencouacq\n",
      "lencouacq<EOS>\n",
      "**************************************************\n",
      "<SOS>le montet\n",
      "le montet<EOS>\n",
      "**************************************************\n",
      "<SOS>villebout\n",
      "villebout<EOS>\n",
      "**************************************************\n",
      "<SOS>irreville\n",
      "irreville<EOS>\n",
      "**************************************************\n",
      "<SOS>recologne\n",
      "recologne<EOS>\n",
      "**************************************************\n",
      "<SOS>insviller\n",
      "insviller<EOS>\n",
      "**************************************************\n",
      "<SOS>neuflieux\n",
      "neuflieux<EOS>\n",
      "**************************************************\n",
      "<SOS>sinnamary\n",
      "sinnamary<EOS>\n",
      "**************************************************\n",
      "<SOS>irissarry\n",
      "irissarry<EOS>\n",
      "**************************************************\n",
      "<SOS>verzeille\n",
      "verzeille<EOS>\n",
      "**************************************************\n",
      "<SOS>le recoux\n",
      "le recoux<EOS>\n",
      "**************************************************\n",
      "<SOS>montayral\n",
      "montayral<EOS>\n",
      "**************************************************\n",
      "<SOS>louvrechy\n",
      "louvrechy<EOS>\n",
      "**************************************************\n",
      "<SOS>voultegon\n",
      "voultegon<EOS>\n",
      "**************************************************\n",
      "<SOS>pellegrue\n",
      "pellegrue<EOS>\n",
      "**************************************************\n",
      "<SOS>brosville\n",
      "brosville<EOS>\n",
      "**************************************************\n",
      "<SOS>seignelay\n",
      "seignelay<EOS>\n",
      "**************************************************\n",
      "<SOS>boissezon\n",
      "boissezon<EOS>\n",
      "**************************************************\n",
      "<SOS>barbières\n",
      "barbières<EOS>\n",
      "**************************************************\n",
      "<SOS>comberjon\n",
      "comberjon<EOS>\n",
      "**************************************************\n",
      "<SOS>peyrecave\n",
      "peyrecave<EOS>\n",
      "**************************************************\n",
      "<SOS>pléboulle\n",
      "pléboulle<EOS>\n",
      "**************************************************\n",
      "<SOS>amblimont\n",
      "amblimont<EOS>\n",
      "**************************************************\n",
      "<SOS>écaquelon\n",
      "écaquelon<EOS>\n",
      "**************************************************\n",
      "<SOS>tressange\n",
      "tressange<EOS>\n",
      "**************************************************\n",
      "<SOS>le manoir\n",
      "le manoir<EOS>\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "for i in range(Xb.shape[0]):\n",
    "    nom_X = \"\"\n",
    "    for id in Xb[i]:\n",
    "        nom_X += int_to_char[int(id.item())]\n",
    "\n",
    "    nom_Y = \"\"\n",
    "    for id in Yb[i]:\n",
    "        nom_Y += int_to_char[int(id.item())]\n",
    "    print(nom_X)\n",
    "    print(nom_Y)\n",
    "    print(\"**************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandretl\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/alex/Bureau/llm/blablateurbinaire/wandb/run-20230710_221917-79y7hnnx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alexandretl/bengio_lm/runs/79y7hnnx' target=\"_blank\">generous-glitter-8</a></strong> to <a href='https://wandb.ai/alexandretl/bengio_lm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alexandretl/bengio_lm' target=\"_blank\">https://wandb.ai/alexandretl/bengio_lm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alexandretl/bengio_lm/runs/79y7hnnx' target=\"_blank\">https://wandb.ai/alexandretl/bengio_lm/runs/79y7hnnx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "BengioLM(\n",
       "  (embed): Embedding(45, 16)\n",
       "  (fc1): Linear(in_features=48, out_features=100, bias=True)\n",
       "  (lm_head): Linear(in_features=100, out_features=45, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#writer = SummaryWriter(log_dir=\"runs/mlp/batch_size=\" + str(batch_size) + \"_lr=\" + str(lr))\n",
    "wandb.init(project=\"bengio_lm\",\n",
    "           config={\n",
    "               \"learning_rate\": lr,\n",
    "               \"batch_size\": batch_size,\n",
    "               \"embed_dim\": embed_dim,\n",
    "               \"hidden_dim\": hidden_dim,\n",
    "               \"context_len\": context_len,\n",
    "               \"data_loading\": data\n",
    "           })\n",
    "\n",
    "model = BengioLM()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m update_num \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(N):\n\u001b[1;32m      7\u001b[0m     Xb, Yb \u001b[39m=\u001b[39m get_batch(\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, batch_size)\n\u001b[0;32m----> 9\u001b[0m     z1, a1, logits \u001b[39m=\u001b[39m model(Xb)\n\u001b[1;32m     11\u001b[0m     loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mcross_entropy(logits, Yb)\n\u001b[1;32m     13\u001b[0m     \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mparameters():\n",
      "File \u001b[0;32m~/miniconda3/envs/torch23/lib/python3.11/site-packages/torch/nn/modules/module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1536\u001b[0m     args \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1538\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1539\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1540\u001b[0m     \u001b[39mfor\u001b[39;00m hook_id, hook \u001b[39min\u001b[39;00m (\n\u001b[1;32m   1541\u001b[0m         \u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1542\u001b[0m         \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1543\u001b[0m     ):\n",
      "Cell \u001b[0;32mIn[9], line 11\u001b[0m, in \u001b[0;36mBengioLM.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 11\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed(x)\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, context_len\u001b[39m*\u001b[39membed_dim)\n\u001b[1;32m     13\u001b[0m     z1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1(x)\n\u001b[1;32m     14\u001b[0m     a1 \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mtanh(z1)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch23/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/torch23/lib/python3.11/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch23/lib/python3.11/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "N = 10000\n",
    "start_time = time.time()\n",
    "\n",
    "wandb.watch(model, log=\"all\")\n",
    "\n",
    "for update_num in range(N):\n",
    "    Xb, Yb = get_batch('train', batch_size)\n",
    "\n",
    "    z1, a1, logits = model(Xb)\n",
    "\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.grad = None\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    # eval : track loss (train & val), update_to_data\n",
    "    if update_num % eval_interval == 0:\n",
    "        to_log = {}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for split in ['train', 'val']:\n",
    "                loss_mean = 0\n",
    "                for i in range(eval_iter):\n",
    "                    Xb, Yb = get_batch(split, batch_size)\n",
    "                    _, _, logits = model(Xb)\n",
    "\n",
    "                    loss_mean += F.cross_entropy(logits, Yb).item()\n",
    "                loss_mean /= eval_iter\n",
    "                to_log[\"loss_\" + split] = loss_mean\n",
    "            model.train()\n",
    "\n",
    "            scalars_dict = {}\n",
    "\n",
    "            for name, p in model.named_parameters():\n",
    "                scalars_dict[name] = (lr*p.grad.std() / p.data.std()).log10().item()\n",
    "        \n",
    "        wandb.log(to_log | {\"update_to_data\": scalars_dict}, step=update_num)\n",
    "\n",
    "end_time = time.time()\n",
    "num_examples_processed = N * batch_size\n",
    "\n",
    "print(\"training throughput = {} examples/s\".format(str(num_examples_processed/(end_time-start_time))))\n",
    "wandb.log({\"training_throughput\": num_examples_processed/(end_time-start_time)})\n",
    "wandb.log({\"params_num\": sum([p.numel() for p in model.parameters()])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">generous-glitter-8</strong> at: <a href='https://wandb.ai/alexandretl/bengio_lm/runs/79y7hnnx' target=\"_blank\">https://wandb.ai/alexandretl/bengio_lm/runs/79y7hnnx</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230710_221917-79y7hnnx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...montol.\n",
      "...naiselphin-di-rouer-shén-le-blans.\n",
      "...houx.\n",
      "...troffes-d'ole-fe-des.\n",
      "...luphe-bon-pergdebotches.\n",
      "...ossies.\n",
      "...le vaivones.\n",
      "...saincourt-sézan.\n",
      "...bois.\n",
      "...soppesse-ssis.\n"
     ]
    }
   ],
   "source": [
    "# sample\n",
    "\n",
    "g = torch.Generator().manual_seed(40 + 7)\n",
    "\n",
    "for _ in range(10):\n",
    "    nom = \"...\"\n",
    "    while nom[-1] != \".\" or len(nom) == 3:\n",
    "        char_moins_3 = nom[-3]\n",
    "        char_moins_2 = nom[-2]\n",
    "        char_moins_1 = nom[-1]\n",
    "\n",
    "        id_moins_3 = char_to_int[char_moins_3]\n",
    "        id_moins_2 = char_to_int[char_moins_2]\n",
    "        id_moins_1 = char_to_int[char_moins_1]\n",
    "\n",
    "        x = torch.asarray([id_moins_3, id_moins_2, id_moins_1]).view(-1, context_len)\n",
    "\n",
    "        Z1 = C[x].view(-1, context_len*16) @ W1 + b1\n",
    "        A1 = torch.tanh(Z1)\n",
    "\n",
    "        Z2 = A1 @ W2 + b2\n",
    "        A2 = F.softmax(Z2, dim=1)\n",
    "\n",
    "        next_id = torch.multinomial(A2, num_samples=1, replacement=True, generator=g).item()\n",
    "        next_char = int_to_char[next_id]\n",
    "\n",
    "        nom = nom + next_char\n",
    "    print(nom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size    # TT    # GPU util\n",
    "#  8            11,5k       32%\n",
    "#  16           23k         32%\n",
    "#  32           45k         32%\n",
    "#  64           86k         36%\n",
    "#  128          168k        40%\n",
    "#  256          330k        36%\n",
    "#  512          640k        38%\n",
    "#  1024         1.14M       46% (optimal)\n",
    "#  2048         1.5M        48%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
