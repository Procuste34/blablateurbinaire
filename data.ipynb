{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ici,\n",
    "#token = char\n",
    "#seq = mot\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "\n",
    "    def __init__(self, words, chars, max_word_length):\n",
    "        #words = ['perols', 'palavas', ...]\n",
    "        #chars = ['a', 'b', 'c', 'd', 'e', 'f', 'g'\n",
    "        \n",
    "        #stoi : char vers int\n",
    "        #itos : int vers char\n",
    "        \n",
    "        self.words = words\n",
    "        self.chars = chars\n",
    "        self.max_word_length = max_word_length\n",
    "        self.stoi = {ch:i+1 for i,ch in enumerate(chars)}\n",
    "        self.itos = {i:s for s,i in self.stoi.items()} # inverse mapping\n",
    "\n",
    "    #nb de séquences (ie mots)\n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "\n",
    "    # test si contient une certaine séquence (ie mot)\n",
    "    def contains(self, word):\n",
    "        return word in self.words\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return len(self.chars) + 1 # all the possible characters and special 0 token\n",
    "\n",
    "    def get_output_length(self):\n",
    "        return self.max_word_length + 1 # <START> token followed by words\n",
    "\n",
    "    def encode(self, word):\n",
    "        ix = torch.tensor([self.stoi[w] for w in word], dtype=torch.long)\n",
    "        return ix\n",
    "\n",
    "    def decode(self, ix):\n",
    "        word = ''.join(self.itos[i] for i in ix)\n",
    "        return word\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        word = self.words[idx]\n",
    "        ix = self.encode(word)\n",
    "        x = torch.zeros(self.max_word_length + 1, dtype=torch.long)\n",
    "        y = torch.zeros(self.max_word_length + 1, dtype=torch.long)\n",
    "        x[1:1+len(ix)] = ix\n",
    "        y[:len(ix)] = ix\n",
    "        y[len(ix)+1:] = -1 # index -1 will mask the loss at the inactive locations\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"villes.txt\", 'r') as f:\n",
    "        data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples in the dataset: 36583\n",
      "max word length: 45\n",
      "number of unique characters in the vocabulary: 43\n",
      "vocabulary:\n",
      " '-abcdefghijklmnopqrstuvwxyzàâçèéêëîïôûüÿœ\n"
     ]
    }
   ],
   "source": [
    "words = data.splitlines()\n",
    "words = [w.strip() for w in words] # get rid of any leading or trailing white space\n",
    "words = [w for w in words if w] # get rid of any empty strings\n",
    "chars = sorted(list(set(''.join(words)))) # all the possible characters\n",
    "max_word_length = max(len(w) for w in words)\n",
    "\n",
    "print(f\"number of examples in the dataset: {len(words)}\")\n",
    "print(f\"max word length: {max_word_length}\")\n",
    "print(f\"number of unique characters in the vocabulary: {len(chars)}\")\n",
    "print(\"vocabulary:\")\n",
    "print(''.join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split up the dataset into 35583 training examples and 1000 test examples\n"
     ]
    }
   ],
   "source": [
    "# partition the input data into a training and the test set\n",
    "test_set_size = min(1000, int(len(words) * 0.1)) # 10% of the training set, or up to 1000 examples\n",
    "rp = torch.randperm(len(words)).tolist()\n",
    "train_words = [words[i] for i in rp[:-test_set_size]]\n",
    "test_words = [words[i] for i in rp[-test_set_size:]]\n",
    "print(f\"split up the dataset into {len(train_words)} training examples and {len(test_words)} test examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CharDataset(train_words, chars, max_word_length)\n",
    "test_dataset = CharDataset(test_words, chars, max_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CharDataset at 0x7f4a72357ad0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfiniteDataLoader:\n",
    "    \"\"\"\n",
    "    this is really hacky and I'm not proud of it, but there doesn't seem to be\n",
    "    a better way in PyTorch to just create an infinite dataloader?\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, **kwargs):\n",
    "        train_sampler = torch.utils.data.RandomSampler(dataset, replacement=True, num_samples=int(1e10))\n",
    "        self.train_loader = DataLoader(dataset, sampler=train_sampler, **kwargs)\n",
    "        self.data_iter = iter(self.train_loader)\n",
    "\n",
    "    def next(self):\n",
    "        try:\n",
    "            batch = next(self.data_iter)\n",
    "        except StopIteration: # this will technically only happen after 1e10 samples... (i.e. basically never)\n",
    "            self.data_iter = iter(self.train_loader)\n",
    "            batch = next(self.data_iter)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mon_loader = InfiniteDataLoader(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = mon_loader.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0, 15,  4,  1, 17,  8, 24, 25, 12, 15, 15,  8,  3, 22, 12, 21,  8,  3,\n",
       "           5,  8, 21, 17,  4, 21,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0]]),\n",
       " tensor([[15,  4,  1, 17,  8, 24, 25, 12, 15, 15,  8,  3, 22, 12, 21,  8,  3,  5,\n",
       "           8, 21, 17,  4, 21,  7,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "          -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
