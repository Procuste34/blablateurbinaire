{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load les données\n",
    "\n",
    "fichier = open('villes.txt')\n",
    "donnees = fichier.read()\n",
    "villes = donnees.replace('\\n', ',').split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparation des données\n",
    "\n",
    "# on rajoute le token . au début et en fin\n",
    "for ville, i in zip(villes, range(len(villes))):\n",
    "    villes[i] = ville + '.'\n",
    "\n",
    "# création du vocabulaire\n",
    "vocabulaire = []\n",
    "\n",
    "for ville in villes:\n",
    "    for c in ville:\n",
    "        if c not in vocabulaire:\n",
    "            vocabulaire.append(c)\n",
    "\n",
    "vocabulaire = sorted(vocabulaire)\n",
    "vocabulaire[0] = '.'\n",
    "vocabulaire[3] = \" \"\n",
    "\n",
    "# pour convertir char <-> int\n",
    "char_to_int = {}\n",
    "int_to_char = {}\n",
    "\n",
    "for (c, i) in zip(vocabulaire, range(len(vocabulaire))):\n",
    "    char_to_int[c] = i\n",
    "    int_to_char[i] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# création du dataset\n",
    "\n",
    "context_len = 8\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for ville in villes:\n",
    "    context = [0] * context_len\n",
    "\n",
    "    for ch in ville:\n",
    "        X.append(context)\n",
    "        Y.append(char_to_int[ch])\n",
    "\n",
    "        context = context[1:] + [char_to_int[ch]]\n",
    "\n",
    "X = torch.tensor(X) # (M, context_len), int64\n",
    "Y = torch.tensor(Y) # (M), int64\n",
    "\n",
    "n1 = int(0.8*X.shape[0])\n",
    "\n",
    "X_train = X[:n1]\n",
    "X_val = X[n1:]\n",
    "\n",
    "Y_train = Y[:n1]\n",
    "Y_val = Y[n1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size, split):\n",
    "    if split == 'train':\n",
    "        ix = torch.randint(X_train.shape[0], (batch_size,))\n",
    "\n",
    "        if device == 'cuda':\n",
    "            Xb = X_train[ix].pin_memory().to(device, non_blocking=True)\n",
    "            Yb = Y_train[ix].pin_memory().to(device, non_blocking=True)\n",
    "        else:\n",
    "            Xb = X_train[ix].to(device)\n",
    "            Yb = Y_train[ix].to(device)\n",
    "    else:\n",
    "        ix = torch.randint(X_val.shape[0], (batch_size,))\n",
    "\n",
    "        if device == 'cuda':\n",
    "            Xb = X_val[ix].pin_memory().to(device, non_blocking=True)\n",
    "            Yb = Y_val[ix].pin_memory().to(device, non_blocking=True)\n",
    "        else:\n",
    "            Xb = X_val[ix].to(device)\n",
    "            Yb = Y_val[ix].to(device)\n",
    "    \n",
    "    return Xb, Yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 10**(-3.901)\n",
    "batch_size = 1024\n",
    "embed_dim = 32\n",
    "hidden_dim = 500\n",
    "\n",
    "eval_interval = 500\n",
    "eval_iter = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BengioLM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed = nn.Embedding(len(vocabulaire), embed_dim)\n",
    "        self.fc1 = nn.Linear(context_len * embed_dim, hidden_dim)\n",
    "\n",
    "        self.lm_head = nn.Linear(hidden_dim, len(vocabulaire))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x).view(-1, context_len*embed_dim)\n",
    "\n",
    "        z1 = self.fc1(x)\n",
    "        a1 = F.tanh(z1)\n",
    "\n",
    "        logits = self.lm_head(a1)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    def sample(self, prompt='', max_new_tokens=None):\n",
    "        prompt = context_len * '.' + prompt\n",
    "        init_len = len(prompt)\n",
    "\n",
    "        while True:\n",
    "            context = prompt[-context_len:]\n",
    "            context_tokenized = [char_to_int[c] for c in context]\n",
    "\n",
    "            logits = self.forward(torch.tensor(context_tokenized, device=device))\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "\n",
    "            next_token = torch.multinomial(probs, num_samples=1, replacement=True).item()\n",
    "            next_char = int_to_char[next_token]\n",
    "\n",
    "            if next_char == '.':\n",
    "                break\n",
    "\n",
    "            prompt += next_char\n",
    "\n",
    "            if len(prompt)-init_len == max_new_tokens:\n",
    "                break\n",
    "        \n",
    "        return prompt[context_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1q20snyt) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss_train</td><td>█▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_val</td><td>█▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>params_num</td><td>▁</td></tr><tr><td>training_throughput</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss_train</td><td>1.5279</td></tr><tr><td>loss_val</td><td>1.62666</td></tr><tr><td>params_num</td><td>151952</td></tr><tr><td>training_throughput</td><td>704585.40058</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">soft-sea-15</strong> at: <a href='https://wandb.ai/alexandretl/bengio_lm/runs/1q20snyt' target=\"_blank\">https://wandb.ai/alexandretl/bengio_lm/runs/1q20snyt</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230714_095622-1q20snyt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1q20snyt). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/alex/Bureau/llm/blablateurbinaire/wandb/run-20230714_101705-lrizt1ou</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alexandretl/bengio_lm/runs/lrizt1ou' target=\"_blank\">sandy-elevator-16</a></strong> to <a href='https://wandb.ai/alexandretl/bengio_lm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alexandretl/bengio_lm' target=\"_blank\">https://wandb.ai/alexandretl/bengio_lm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alexandretl/bengio_lm/runs/lrizt1ou' target=\"_blank\">https://wandb.ai/alexandretl/bengio_lm/runs/lrizt1ou</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/alexandretl/bengio_lm/runs/lrizt1ou?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f81191f1690>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"bengio_lm\",\n",
    "           config={\n",
    "               \"log_learning_rate\": np.log10(lr),\n",
    "               \"batch_size\": batch_size,\n",
    "               \"embed_dim\": embed_dim,\n",
    "               \"hidden_dim\": hidden_dim,\n",
    "               \"context_len\": context_len\n",
    "           })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BengioLM()\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training throughput = 647831.5823559229 examples/s\n"
     ]
    }
   ],
   "source": [
    "N = 20000\n",
    "start_time = time.time()\n",
    "\n",
    "wandb.watch(model, log=\"all\")\n",
    "\n",
    "for update_num in range(N):\n",
    "    Xb, Yb = get_batch(batch_size, 'train')\n",
    "\n",
    "    logits = model(Xb)\n",
    "\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # eval : track loss (train & val), update_to_data\n",
    "    if update_num % eval_interval == 0:\n",
    "        to_log = {}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for split in ['train', 'val']:\n",
    "                loss_mean = 0\n",
    "                for i in range(eval_iter):\n",
    "                    Xb, Yb = get_batch(batch_size, split)\n",
    "                    logits = model(Xb)\n",
    "\n",
    "                    loss_mean += F.cross_entropy(logits, Yb).item()\n",
    "                loss_mean /= eval_iter\n",
    "                to_log[\"loss_\" + split] = loss_mean\n",
    "            model.train()\n",
    "\n",
    "            scalars_dict = {}\n",
    "\n",
    "            for name, p in model.named_parameters():\n",
    "                scalars_dict[name] = (lr*p.grad.std() / p.data.std()).log10().item()\n",
    "        \n",
    "        wandb.log(to_log | {\"update_to_data\": scalars_dict}, step=update_num)\n",
    "\n",
    "end_time = time.time()\n",
    "num_examples_processed = N * batch_size\n",
    "\n",
    "print(\"training throughput = {} examples/s\".format(str(num_examples_processed/(end_time-start_time))))\n",
    "wandb.log({\"training_throughput\": num_examples_processed/(end_time-start_time)})\n",
    "wandb.log({\"params_num\": sum([p.numel() for p in model.parameters()])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss_train</td><td>█▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_val</td><td>█▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>params_num</td><td>▁</td></tr><tr><td>training_throughput</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss_train</td><td>1.5208</td></tr><tr><td>loss_val</td><td>1.63492</td></tr><tr><td>params_num</td><td>151952</td></tr><tr><td>training_throughput</td><td>647831.58236</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sandy-elevator-16</strong> at: <a href='https://wandb.ai/alexandretl/bengio_lm/runs/lrizt1ou' target=\"_blank\">https://wandb.ai/alexandretl/bengio_lm/runs/lrizt1ou</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230714_101705-lrizt1ou/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lousney-sur-terne\n",
      "axzeneur\n",
      "fontaine-morage\n",
      "gueugny\n",
      "broch\n",
      "cumoise-de-brioules-sur-valors\n",
      "brévèque\n",
      "saint-denis\n",
      "pellac\n",
      "l'écourt\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(model.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo : voir si on a aussi ce deuxième pic avec nos noms de villes générés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size    # TT    # GPU util\n",
    "#  8            11,5k       32%\n",
    "#  16           23k         32%\n",
    "#  32           45k         32%\n",
    "#  64           86k         36%\n",
    "#  128          168k        40%\n",
    "#  256          330k        36%\n",
    "#  512          640k        38%\n",
    "#  1024         1.14M       46% (optimal)\n",
    "#  2048         1.5M        48%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
