{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load les données\n",
    "\n",
    "fichier = open('villes.txt')\n",
    "donnees = fichier.read()\n",
    "villes = donnees.replace('\\n', ',').split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparation des données\n",
    "\n",
    "# on rajoute le token . au début et en fin\n",
    "for ville, i in zip(villes, range(len(villes))):\n",
    "    villes[i] = ville + '.'\n",
    "\n",
    "# création du vocabulaire\n",
    "vocabulaire = []\n",
    "\n",
    "for ville in villes:\n",
    "    for c in ville:\n",
    "        if c not in vocabulaire:\n",
    "            vocabulaire.append(c)\n",
    "\n",
    "vocabulaire = sorted(vocabulaire)\n",
    "vocabulaire[0] = '.'\n",
    "vocabulaire[3] = \" \"\n",
    "\n",
    "# pour convertir char <-> int\n",
    "char_to_int = {}\n",
    "int_to_char = {}\n",
    "\n",
    "for (c, i) in zip(vocabulaire, range(len(vocabulaire))):\n",
    "    char_to_int[c] = i\n",
    "    int_to_char[i] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arbignieu.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "villes[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...---> a\n",
      "..a---> r\n",
      ".ar---> b\n",
      "arb---> i\n",
      "rbi---> g\n",
      "big---> n\n",
      "ign---> i\n",
      "gni---> e\n",
      "nie---> u\n",
      "ieu---> .\n"
     ]
    }
   ],
   "source": [
    "context_len = 3\n",
    "\n",
    "for ville in villes[:1]:\n",
    "    context = [0] * context_len\n",
    "\n",
    "    for ch in ville:\n",
    "        print(''.join([int_to_char[p] for p in context]) + \"---> \" + ch)\n",
    "\n",
    "        context = context[1:] + [char_to_int[ch]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# création du dataset\n",
    "\n",
    "context_len = 3\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for ville in villes:\n",
    "    context = [0] * context_len\n",
    "\n",
    "    for ch in ville:\n",
    "        X.append(context)\n",
    "        Y.append(char_to_int[ch])\n",
    "\n",
    "        context = context[1:] + [char_to_int[ch]]\n",
    "\n",
    "\n",
    "X = torch.tensor(X) # (M, 3), int64\n",
    "Y = torch.tensor(Y) # (M), int64\n",
    "\n",
    "n1 = int(0.8*X.shape[0])\n",
    "\n",
    "X_train = X[:n1]\n",
    "X_test = X[n1:]\n",
    "\n",
    "Y_train = Y[:n1]\n",
    "Y_test= Y[n1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size, split):\n",
    "    if split == 'train':\n",
    "        ix = torch.randint(X_train.shape[0], (batch_size,))\n",
    "\n",
    "        if device == 'cuda':\n",
    "            Xb = X_train[ix].pin_memory().to(device, non_blocking=True)\n",
    "            Yb = Y_train[ix].pin_memory().to(device, non_blocking=True)\n",
    "        else:\n",
    "            Xb = X_train[ix].to(device)\n",
    "            Yb = Y_train[ix].to(device)\n",
    "    else:\n",
    "        ix = torch.randint(X_test.shape[0], (batch_size,))\n",
    "\n",
    "        if device == 'cuda':\n",
    "            Xb = X_test[ix].pin_memory().to(device, non_blocking=True)\n",
    "            Yb = Y_test[ix].pin_memory().to(device, non_blocking=True)\n",
    "        else:\n",
    "            Xb = X_test[ix].to(device)\n",
    "            Yb = Y_test[ix].to(device)\n",
    "    \n",
    "    return Xb, Yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.03\n",
    "batch_size = 1024\n",
    "embed_dim = 16\n",
    "hidden_dim = 100\n",
    "\n",
    "eval_interval = 500\n",
    "eval_iter = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BengioLM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed = nn.Embedding(len(vocabulaire), embed_dim)\n",
    "        self.fc1 = nn.Linear(context_len * embed_dim, hidden_dim)\n",
    "\n",
    "        self.lm_head = nn.Linear(hidden_dim, len(vocabulaire))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x).view(-1, context_len*embed_dim)\n",
    "\n",
    "        z1 = self.fc1(x)\n",
    "        a1 = F.tanh(z1)\n",
    "\n",
    "        logits = self.lm_head(a1)\n",
    "\n",
    "        return z1, a1, logits\n",
    "    \n",
    "    def sample(self, prompt, max_new_tokens):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexandretl\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/alex/Bureau/llm/blablateurbinaire/wandb/run-20230713_190729-eyyfc04i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alexandretl/bengio_lm/runs/eyyfc04i' target=\"_blank\">breezy-frost-9</a></strong> to <a href='https://wandb.ai/alexandretl/bengio_lm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alexandretl/bengio_lm' target=\"_blank\">https://wandb.ai/alexandretl/bengio_lm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alexandretl/bengio_lm/runs/eyyfc04i' target=\"_blank\">https://wandb.ai/alexandretl/bengio_lm/runs/eyyfc04i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "BengioLM(\n",
       "  (embed): Embedding(44, 16)\n",
       "  (fc1): Linear(in_features=48, out_features=100, bias=True)\n",
       "  (lm_head): Linear(in_features=100, out_features=44, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#writer = SummaryWriter(log_dir=\"runs/mlp/batch_size=\" + str(batch_size) + \"_lr=\" + str(lr))\n",
    "wandb.init(project=\"bengio_lm\",\n",
    "           config={\n",
    "               \"learning_rate\": lr,\n",
    "               \"batch_size\": batch_size,\n",
    "               \"embed_dim\": embed_dim,\n",
    "               \"hidden_dim\": hidden_dim,\n",
    "               \"context_len\": context_len\n",
    "           })\n",
    "\n",
    "model = BengioLM()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training throughput = 941639.2118829766 examples/s\n"
     ]
    }
   ],
   "source": [
    "N = 10000\n",
    "start_time = time.time()\n",
    "\n",
    "wandb.watch(model, log=\"all\")\n",
    "\n",
    "for update_num in range(N):\n",
    "    Xb, Yb = get_batch(batch_size, 'train')\n",
    "\n",
    "    z1, a1, logits = model(Xb)\n",
    "\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.grad = None\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    # eval : track loss (train & val), update_to_data\n",
    "    if update_num % eval_interval == 0:\n",
    "        to_log = {}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for split in ['train', 'val']:\n",
    "                loss_mean = 0\n",
    "                for i in range(eval_iter):\n",
    "                    Xb, Yb = get_batch(batch_size, split)\n",
    "                    _, _, logits = model(Xb)\n",
    "\n",
    "                    loss_mean += F.cross_entropy(logits, Yb).item()\n",
    "                loss_mean /= eval_iter\n",
    "                to_log[\"loss_\" + split] = loss_mean\n",
    "            model.train()\n",
    "\n",
    "            scalars_dict = {}\n",
    "\n",
    "            for name, p in model.named_parameters():\n",
    "                scalars_dict[name] = (lr*p.grad.std() / p.data.std()).log10().item()\n",
    "        \n",
    "        wandb.log(to_log | {\"update_to_data\": scalars_dict}, step=update_num)\n",
    "\n",
    "end_time = time.time()\n",
    "num_examples_processed = N * batch_size\n",
    "\n",
    "print(\"training throughput = {} examples/s\".format(str(num_examples_processed/(end_time-start_time))))\n",
    "wandb.log({\"training_throughput\": num_examples_processed/(end_time-start_time)})\n",
    "wandb.log({\"params_num\": sum([p.numel() for p in model.parameters()])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss_train</td><td>█▄▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_val</td><td>█▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>params_num</td><td>▁</td></tr><tr><td>training_throughput</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss_train</td><td>2.02803</td></tr><tr><td>loss_val</td><td>2.03244</td></tr><tr><td>params_num</td><td>10048</td></tr><tr><td>training_throughput</td><td>941639.21188</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">breezy-frost-9</strong> at: <a href='https://wandb.ai/alexandretl/bengio_lm/runs/eyyfc04i' target=\"_blank\">https://wandb.ai/alexandretl/bengio_lm/runs/eyyfc04i</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230713_190729-eyyfc04i/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...montol.\n",
      "...naiselphin-di-rouer-shén-le-blans.\n",
      "...houx.\n",
      "...troffes-d'ole-fe-des.\n",
      "...luphe-bon-pergdebotches.\n",
      "...ossies.\n",
      "...le vaivones.\n",
      "...saincourt-sézan.\n",
      "...bois.\n",
      "...soppesse-ssis.\n"
     ]
    }
   ],
   "source": [
    "# sample\n",
    "\n",
    "g = torch.Generator().manual_seed(40 + 7)\n",
    "\n",
    "for _ in range(10):\n",
    "    nom = \"...\"\n",
    "    while nom[-1] != \".\" or len(nom) == 3:\n",
    "        char_moins_3 = nom[-3]\n",
    "        char_moins_2 = nom[-2]\n",
    "        char_moins_1 = nom[-1]\n",
    "\n",
    "        id_moins_3 = char_to_int[char_moins_3]\n",
    "        id_moins_2 = char_to_int[char_moins_2]\n",
    "        id_moins_1 = char_to_int[char_moins_1]\n",
    "\n",
    "        x = torch.asarray([id_moins_3, id_moins_2, id_moins_1]).view(-1, context_len)\n",
    "\n",
    "        Z1 = C[x].view(-1, context_len*16) @ W1 + b1\n",
    "        A1 = torch.tanh(Z1)\n",
    "\n",
    "        Z2 = A1 @ W2 + b2\n",
    "        A2 = F.softmax(Z2, dim=1)\n",
    "\n",
    "        next_id = torch.multinomial(A2, num_samples=1, replacement=True, generator=g).item()\n",
    "        next_char = int_to_char[next_id]\n",
    "\n",
    "        nom = nom + next_char\n",
    "    print(nom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size    # TT    # GPU util\n",
    "#  8            11,5k       32%\n",
    "#  16           23k         32%\n",
    "#  32           45k         32%\n",
    "#  64           86k         36%\n",
    "#  128          168k        40%\n",
    "#  256          330k        36%\n",
    "#  512          640k        38%\n",
    "#  1024         1.14M       46% (optimal)\n",
    "#  2048         1.5M        48%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
